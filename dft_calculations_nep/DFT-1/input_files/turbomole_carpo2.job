#!/bin/bash -l
#SBATCH -p normal
#SBATCH -n 20                  # number of tasks (MPI/SMP processes)
#SBATCH -N 1                   # for SMP only 1 is possible
##SBATCH  --ntasks-per-node=1  #
#SBATCH -J C60
##SBATCH --exclusive # try to always allocate full nodes, will not go to nodes where something else!!!
#SBATCH -e jobfile.err%J
#SBATCH -o jobfile.out%J
#SBATCH -t 99-00:00:00         #  time as d-hh:mm:ss
#SBATCH --mem-per-cpu=4000     # requested memory per process in MB
##SBATCH --mem=128000          # requested memory per node  in MB, node of carpo is somehting like this
##SBATCH --nodelist=ca[1]
#SBATCH --x=ca-gpu1

module load turbomole2024

SDIR=`pwd`
echo "Submission directory is: $SDIR"
echo "The job ID assigned by the Batch system is: $LSB_JOBID"
echo "Number of requested processes $SLURM_NPROCS"
# MOST OF THE TURBOMOLE IS SMP PARALLEL (IN ONE NODE) BUT IF YOU NEED MORE THAN ONE NODE USE "MPI" HERE 
 export PARA_ARCH=SMP
# export PARA_ARCH=MPI
ulimit -s unlimited
export PARNODES=$SLURM_NPROCS
export PATH=$TURBODIR/bin/`$TURBODIR/scripts/sysname`:$PATH

 export TURBOTMPDIR=/scratch/$USER/tmtmp_${SLURM_JOBID}
# export TURBOTMPDIR=/local/$USER/tmtmp_${SLURM_JOBID}

mkdir $TURBOTMPDIR

# jobbsse > bsserun.out
 ridft > ridft.out
 mpshift > mpshift.out
 rdgrad > rdgrad.out
# escf > escf.out
# jobex  -ri -c 100 -energy 5 -gcart 2 > jobex.out
# aoforce > force.out
